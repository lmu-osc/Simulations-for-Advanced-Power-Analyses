[
  {
    "objectID": "GLMM.html",
    "href": "GLMM.html",
    "title": "GLMM",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "SEM.html",
    "href": "SEM.html",
    "title": "SEM",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "GLM.html",
    "href": "GLM.html",
    "title": "GLM",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "check alpha so your models don‚Äôt yield more than 5% false-positive results\ncheck beta (power) for easy tests such as t-tests (where this isn‚Äôt really needed)\nprepare a preregistration and make sure your code works\ncheck your understanding of statistics\n\nLet‚Äôs dive deeper into power calculations for different complex models.\nFor each, we will follow the structure:\n\ndefine what type of data and variables need to be simulated, i.e.¬†their distribution, their class (e.g.¬†factor vs numerical value), sample sizes (within a dataset, and number of replicates), what will need to vary (e.g.¬†the strength of relationship)\ngenerate data, random data or data including an effect (e.g.¬†an imposed correlation between two variables)\nrun the statistical test you think is appropriate, and record the relevant statistic (e.g.¬†p-value)\nreplicate step 2 and 3 to get the distribution of the statistic of interest\ntry out different parameter sets (explore the parameter space for which results are similar)\nanalyze and interpret the combined results of many simulations within each set of parameters. For instance, check that you only get a significant result in 5% of the simulations (if alpha = 0.05) when you simulated no effect; and that you get at a significant result in 80% of the simulations (if you targeted a power of 80%) when you simulated an effect\n\nHere are the type of models we will cover, you can pick and choose what is relevant to you!\n\n[LM](LM.qmd)\nGLM\nLMM\nGLMM\nSEM\n\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "LMM.html",
    "href": "LMM.html",
    "title": "LMM",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "LM.html",
    "href": "LM.html",
    "title": "Linear Model (a single predictor)",
    "section": "",
    "text": "# TODO: collect the installation of all necessary packages in one place at the beginning of the tutorial\n\n#install.packages(c(\"ggplot2\", \"ggdist\", \"pwr\"))\nWe start with the simplest possible linear model: (a) a continuous outcome variable is predicted by a single dichotomous predictor. This model actually rephrases a t-test as a linear model! Then we build up increasingly complex models: (b) a single continuous predictor and (c) multiple continuous predictors (i.e., multiple regression)."
  },
  {
    "objectID": "LM.html#get-some-real-bdi-data-as-starting-point",
    "href": "LM.html#get-some-real-bdi-data-as-starting-point",
    "title": "Linear Model (a single predictor)",
    "section": "Get some real BDI data as starting point",
    "text": "Get some real BDI data as starting point\n\n\n\n\n\n\nNote\n\n\n\nThe creators of this tutorial are no experts in clinical psychology; we opportunistically selected open data sets based on their availability. Usually, we would look for meta-analyses - ideally bias-corrected - for more comprehensive evidence.\n\n\nThe R package HSAUR contains open data on 100 depressive patients, where 50 received treatment-as-usual (TAU) and 50 received a new treatment (‚ÄúBeat the blues‚Äù; BtheB). Data was collected in a pre-post-design with several follow-up measurements. For the moment, we focus on the pre-treatment baseline value (bdi.pre) and the first post-treatment value (bdi.2m). We will use that data set as a ‚Äúpilot study‚Äù for our power analysis.\n\n# the data can be found in the HSAUR package, must be installed first\n#install.packages(\"HSAUR\")\n\n# load the data\ndata(\"BtheB\", package = \"HSAUR\")\n\n# get some information about the data set:\n?HSAUR::BtheB\n\nhist(BtheB$bdi.pre)\n\n\n\n\nThe standardized cutoffs for the BDI are:\n\n0‚Äì13: minimal depression\n14‚Äì19: mild depression\n20‚Äì28: moderate depression\n29‚Äì63: severe depression.\n\n\ntable(BtheB$bdi.pre >= 14) |> prop.table() |> round(2)\n\n\nFALSE  TRUE \n  0.2   0.8 \n\n\nHence, 80% of participants had at least a ‚Äúmild depression‚Äù before treatment according to these cutoffs.\nReturning to our questions from above:\nWhat BDI values would we expect on average in our sample (before treatment)?\n\n# we take the pre-score here:\nmean(BtheB$bdi.pre)\n\n[1] 23.33\n\n\nThe average BDI score in that sample was 23, corresponding to a ‚Äúmoderate depression‚Äù.\n\nWhat variability would we expect in our sample?\n\n\nvar(BtheB$bdi.pre)\n\n[1] 117.5163\n\n\n\nWhat average treatment effect would we expect?\n\n\n# we take the 2 month follow-up measurement, \n# separately for the  \"treatment as usual\" and \n# the \"Beat the blues\" group:\nmean(BtheB$bdi.2m[BtheB$treatment == \"TAU\"], na.rm=TRUE)\n\n[1] 19.46667\n\nmean(BtheB$bdi.2m[BtheB$treatment == \"BtheB\"])\n\n[1] 14.71154\n\n\nHence, the two treatments reduced BDI scores from an average of 23 to 19 (TAU) and 15 (BtheB). Based on that data set, we can conclude that a typical treatment effect is somewhere between a 4 and a 8-point reduction of BDI scores.1\nFor our purpose, we compute the average treatment effect combined for both treatments. The average post-treatment score is:\n\nmean(BtheB$bdi.2m, na.rm=TRUE)\n\n[1] 16.91753\n\n\nSo, the average reduction across both treatments is \\(17-23=6\\)."
  },
  {
    "objectID": "LM.html#enter-specific-values-for-the-model-parameters",
    "href": "LM.html#enter-specific-values-for-the-model-parameters",
    "title": "Linear Model (a single predictor)",
    "section": "Enter specific values for the model parameters",
    "text": "Enter specific values for the model parameters\nLet‚Äôs rewrite the abstract equation with the specific variable names. We first write the equation for the systematic part (without the error term). This also represents the predicted value:\n\\[\\widehat{\\text{BDI}} = b_0 + b_1*\\text{treatment} \\tag{2}\\]\nWe use the notation \\(\\widehat{\\text{BDI}}\\) (with a hat) to denote the predicted BDI score.\nThe predicted score for the control group then simply is the intercept of the model, as the second term is erased by entering the value ‚Äú0‚Äù for the control group:\n\\[\\widehat{\\text{BDI}} = b_0 + b_1*0 = b_0\\]\nThe predicted score for the treatment group is the value for the control group plus the regression weight:\n\\[\\widehat{\\text{BDI}} = b_0 + b_1*1\\] Hence, the regression weight (aka. ‚Äúslope parameter‚Äù) \\(b_1\\) estimates the mean difference between both groups, which is the treatment effect.\nWith our knowledge from the open BDI data, we insert plausible values for the intercept \\(b_0\\) and the treatment effect \\(b_1\\). We expect a reduction of the depression score, so the treatment effect is assumed to be negative. We take the combined treatment effect of the two pilot treatments. And as power analysis is not rocket science, we generously round the values:\n\\[\\widehat{\\text{BDI}} = 23 - 6*treatment\\]\nHence, the predicted value is \\(23 - 6*0 = 23\\) for the control group, and \\(23 - 6*1 = 17\\) for the treatment group.\nWith the current model, all persons in the control group have the same predicted value (23), as do all persons in the treatment group (17).\nAs a final step, we add the random noise to the model, based on the variance in the pilot data:\n\\[\\text{BDI} = 23 - 6*treatment + e; e \\sim N(0, var=117) \\]\nThat‚Äôs our final equation with assumed population parameters! With that equation, we assume a certain state of reality and can sample ‚Äúvirtual participants‚Äù."
  },
  {
    "objectID": "LM.html#what-is-the-effect-size-in-the-model",
    "href": "LM.html#what-is-the-effect-size-in-the-model",
    "title": "Linear Model (a single predictor)",
    "section": "What is the effect size in the model?",
    "text": "What is the effect size in the model?\nThe raw effect size is simply the treatment effect on the original BDI scale (i.e., the group difference in the outcome variable). In our case we assume that the treatment lowers the BDI score by 6 points, on average. The standardized effect size relates the raw effect size to the variability. In the two-group example, this can be expressed as Cohen‚Äôs d, which is the mean difference divided by the standard deviation (SD):\n\\[d = \\frac{M_{treat} - M_{control}}{SD} = \\frac{17 - 23}{\\sqrt{117}} = -0.55\\]\n\n\n\n\n\n\nNote\n\n\n\nIf you look up the formula of Cohen‚Äôs d, it typically uses the pooled SD from both groups. As we assumed that both groups have the same SD, we simply took that value."
  },
  {
    "objectID": "LM.html#doing-the-power-analysis",
    "href": "LM.html#doing-the-power-analysis",
    "title": "Linear Model (a single predictor)",
    "section": "Doing the power analysis",
    "text": "Doing the power analysis\nNow we need to repeatedly draw many samples and see how many of the analyses would have detected the existing effect. To do this, we put the code from above into a loop and repeatedly do the procedure.\n\nset.seed(0xBEEF)\n\n# define all predictor and simulation variables.\n# (we do this outside the loop as it stays constant)\niterations <- 1000\nn <- 100\ntreatment <- c(rep(0, n/2), rep(1, n/2))\n\n# vector that collects the p-values from all 1000 virtual samples\n# (prepare an empty NA vector with 1000 slots)\np_values <- rep(NA, iterations)\n\n# now repeatedly draw samples, analyze, and the p-value of the \n# focal regression weight (i.e., the slope parameter)\nfor (i in 1:iterations) {\n  BDI <- 23 - 6*treatment + rnorm(n, mean=0, sd=sqrt(117))\n  res <- lm(BDI ~ treatment)\n  p_values[i] <- summary(res)$coefficients[\"treatment\", \"Pr(>|t|)\"]\n}\n\nHow many of our 1000 virtual samples would have found the effect?\n\ntable(p_values < .005)\n\n\nFALSE  TRUE \n  535   465 \n\n\nOnly 46% of samples with the same size of \\(n=100\\) result in a significant p-value.\n46% - that is our power for \\(\\alpha = .005\\), Cohen‚Äôs \\(d=.55\\), and \\(n=100\\)."
  },
  {
    "objectID": "LM.html#sample-size-planning-find-the-necessary-sample-size",
    "href": "LM.html#sample-size-planning-find-the-necessary-sample-size",
    "title": "Linear Model (a single predictor)",
    "section": "Sample size planning: Find the necessary sample size",
    "text": "Sample size planning: Find the necessary sample size\nNow we know that a sample size of 100 does not lead to a sufficient power. But what sample size would we need to achieve a power of at least 80%? In the simulation approach you need to test different \\(n\\)s until you find the necessary sample size. We do this by wrapping the simulation code into another loop that continuously increases the n.¬†We then store the computed power for each n.\n\nset.seed(0xBEEF)\n\n# define all predictor and simulation variables.\niterations <- 1000\nns <- seq(100, 300, by=20) # test ns between 100 and 400\n\nresult <- data.frame()\n\nfor (n in ns) {  # outer loop\n  treatment <- c(rep(0, n/2), rep(1, n/2))\n  p_values <- rep(NA, iterations)\n\n  # now repeatedly draw samples, analyze, and save p-value\n  for (i in 1:iterations) {  # inner loop\n    BDI <- 23 - 6*treatment + rnorm(n, mean=0, sd=sqrt(117))\n    res <- lm(BDI ~ treatment)\n    p_values[i] <- summary(res)$coefficients[\"treatment\", \"Pr(>|t|)\"]\n  } \n  \n  result <- rbind(result, data.frame(\n    n = n,\n    power = sum(p_values < .005)/iterations)\n  )\n  \n  # show the result after each run (not shown here in the tutorial)\n  print(result)\n}\n\nLet‚Äôs plot there result:\n\nggplot(result, aes(x=n, y=power)) + geom_point() + geom_line()\n\n\n\n\nHence, with n=180 (90 in each group), we have a 80% chance to detect the effect.\nü•≥ Congratulations! You did your first power analysis by simulation. üéâ\nFor these simple models, we can also compute analytic solutions. Let‚Äôs verify our results with the pwr package:\n\nlibrary(pwr)\npwr.t.test(d = 0.55, sig.level = 0.005, power = .80)\n\n\n     Two-sample t test power calculation \n\n              n = 90.00212\n              d = 0.55\n      sig.level = 0.005\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nExactly the same result - phew üòÖ"
  },
  {
    "objectID": "LM.html#safeguard-power-analysis",
    "href": "LM.html#safeguard-power-analysis",
    "title": "Linear Model (a single predictor)",
    "section": "Safeguard power analysis",
    "text": "Safeguard power analysis\nAs sensitivity analysis, we will apply a safeguard power analysis, that aims for the lower end of a two-sided 60% CI around the parameter of the focal treatment effect (the intercept is irrelevant). (Of course you can use any other value than 60%, but this is the value (tentatively) mentioned by the inventors of the safeguard power analysis.)\n\n\n\n\n\n\nNote\n\n\n\nIf you assume publication bias, another heuristic for aiming at a more realistic population effect size is the ‚Äúdivide-by-2‚Äù heuristic. (TODO: Link to presentation)\n\n\nWe can use the t.test function to compute a CI around that value:\n\nt.test(BtheB$bdi.2m, BtheB$bdi.pre, na.rm=TRUE, conf.level = 0.60)\n\n\n    Welch Two Sample t-test\n\ndata:  BtheB$bdi.2m and BtheB$bdi.pre\nt = -4.1613, df = 194.87, p-value = 4.745e-05\nalternative hypothesis: true difference in means is not equal to 0\n60 percent confidence interval:\n -7.712244 -5.112704\nsample estimates:\nmean of x mean of y \n 16.91753  23.33000 \n\n\n\n\n\n\n\n\nMOTE Online Calculator\n\n\n\nYou can also use the MOTE online calculator by Erin Buchanan et al.¬†to compute effect sizes and confidence intervals around them: https://doomlab.shinyapps.io/mote/\nThis takes either raw statistics (means, SDs) or test statistics as input.\n\n\nAs the assumed effect is negative, we aim for the upper, i.e., the more conservative limit, which is considerably smaller, at -5.1.\n\n\n\n\n\n\nTip: Computing CIs around standardized effect sizes\n\n\n\nWhen you use a standardized mean difference that has been reported in a paper, you can use a function from the MBESS package to compute the CI:\nlibrary(MBESS)\n\n# smd = standardized mean difference = Cohen's d\nci.smd(smd=0.55, n.1=50, n.2=50, conf.level=0.60)\n\n\nNow we can rerun the power simulation with this more conservative value (the only change to the code above is that we changed the treatment effect from -6 to -5.1).\n\n\nShow the code\nset.seed(0xBEEF)\n\n# define all predictor and simulation variables.\niterations <- 1000\nns <- seq(100, 300, by=20)\n\nresult <- data.frame()\n\nfor (n in ns) {\n  treatment <- c(rep(0, n/2), rep(1, n/2))\n  p_values <- rep(NA, iterations)\n\n  # now repeatedly draw samples, analyze, and save p-value of \n  for (i in 1:iterations) {\n    BDI <- 23 - 5.1*treatment + rnorm(n, mean=0, sd=sqrt(117))\n    res <- lm(BDI ~ treatment)\n    p_values[i] <- summary(res)$coefficients[\"treatment\", \"Pr(>|t|)\"]\n  } \n  \n  result <- rbind(result, data.frame(\n    n = n,\n    power = sum(p_values < .005)/iterations)\n  )\n  \n  # show the result after each run\n  print(result)\n}\n\n\n\n\n     n power\n1  100 0.314\n2  120 0.408\n3  140 0.473\n4  160 0.570\n5  180 0.618\n6  200 0.717\n7  220 0.735\n8  240 0.795\n9  260 0.823\n10 280 0.878\n11 300 0.887\n\n\nWith that more conservative effect size assumption, we would need around 240 participants, i.e.¬†120 per group."
  },
  {
    "objectID": "LM.html#smallest-effect-size-of-interest-sesoi",
    "href": "LM.html#smallest-effect-size-of-interest-sesoi",
    "title": "Linear Model (a single predictor)",
    "section": "Smallest effect size of interest (SESOI)",
    "text": "Smallest effect size of interest (SESOI)\nMany methodologists argue that we should not power for the expected effects size, but rather for the smallest effect size of interest. In this case, a non-significant result can be interpreted as ‚ÄúWe accept the \\(H_0\\), and even if a real effect existed, it most likely is too small to be relevant‚Äù.\nWhat change of BDI scores is perceived as ‚Äúclinically important‚Äù? The hard part is to find a convincing theoretical or empirical argument for the chosen SESOI. In the case of the BDI, luckily someone else did that work.\nThe NICE guidance suggest that a change of >=3 BDI-II points is clinically important.\nHowever, as you can expect, things are more complicated. Button et al.¬†(2015) analyzed data sets where patients have been asked, after a treatment, whether they felt ‚Äúbetter‚Äù, ‚Äúthe same‚Äù or ‚Äúworse‚Äù. With these subjective ratings, they could relate changes in BDI-II scores to perceived improvements. Hence, even when depressive symptoms where measureably reduced in the BDI, patients still might answer ‚Äúfeels the same‚Äù, which indicates that the reduction did not surpass a threshold of subjective relevant improvement. For example, the minimal clinical importance depends on the baseline severity: For patients to feel notably better, they need more reduction of BDI-II scores if they start from a higher level of depressive symptoms. Following from this analysis, typical SESOIs are higher than the NICE guidelines, more in the range of -6 BDI points.\nLet‚Äôs use the NICE recommendation of -3 BDI points as a lower threshold for our power analysis (anything larger than that will be covered anyway).\n\n\nShow the code\nset.seed(0xBEEF)\n\n# define all predictor and simulation variables.\niterations <- 1000\nns <- seq(600, 800, by=20)\n\nresult <- data.frame()\n\nfor (n in ns) {\n  treatment <- c(rep(0, n/2), rep(1, n/2))\n  p_values <- rep(NA, iterations)\n\n  # now repeatedly draw samples, analyze, and save p-value of \n  for (i in 1:iterations) {\n    BDI <- 23 - 3*treatment + rnorm(n, mean=0, sd=sqrt(117))\n    res <- lm(BDI ~ treatment)\n    p_values[i] <- summary(res)$coefficients[\"treatment\", \"Pr(>|t|)\"]\n  } \n  \n  result <- rbind(result, data.frame(\n    n = n,\n    power = sum(p_values < .005)/iterations)\n  )\n}\n\n\n\n\n     n power\n1  600 0.728\n2  620 0.738\n3  640 0.756\n4  660 0.781\n5  680 0.791\n6  700 0.791\n7  720 0.827\n8  740 0.820\n9  760 0.852\n10 780 0.866\n11 800 0.871\n\n\nHence, we need around 700 participants to reliably detect this smallest effect size of interest.\nDid you spot the strange pattern in the result? At n=720, the power is 83%, but only 82% with n=740? This is not possible and suggests that this is simply Monte Carlo sampling error - 1000 iterations is not enough to get precise estimates. When we increase iterations to 10,000, it takes much longer, but gives more precise results:\n\n\nShow the code\nset.seed(0xBEEF)\n\n# define all predictor and simulation variables.\niterations <- 10000\n\n# To speed up computations, we limited the range of ns to the relevant region\nns <- seq(640, 740, by=20)\n\nresult <- data.frame()\n\nfor (n in ns) {\n  treatment <- c(rep(0, n/2), rep(1, n/2))\n  p_values <- rep(NA, iterations)\n\n  # now repeatedly draw samples, analyze, and save p-value of \n  for (i in 1:iterations) {\n    BDI <- 23 - 3*treatment + rnorm(n, mean=0, sd=sqrt(117))\n    res <- lm(BDI ~ treatment)\n    p_values[i] <- summary(res)$coefficients[\"treatment\", \"Pr(>|t|)\"]\n  } \n  \n  result <- rbind(result, data.frame(\n    n = n,\n    power = sum(p_values < .005)/iterations)\n  )\n  \n  # show the result after each run\n  print(result)\n}\n\n\n\n\n     n  power\n1  600 0.7244\n2  620 0.7353\n3  640 0.7528\n4  660 0.7721\n5  680 0.7889\n6  700 0.8001\n7  720 0.8150\n8  740 0.8328\n9  760 0.8374\n10 780 0.8573\n11 800 0.8682"
  },
  {
    "objectID": "Resources.html",
    "href": "Resources.html",
    "title": "Resources",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  }
]